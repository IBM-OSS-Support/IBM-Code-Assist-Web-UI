2025/04/24 21:56:56 routes.go:1232: INFO server config env="map[HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:2048 OLLAMA_DEBUG:true OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://127.0.0.1:11434 OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/Users/sachinsuresh/.ollama/models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:0 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_SCHED_SPREAD:false http_proxy: https_proxy: no_proxy:]"
time=2025-04-24T21:56:56.893+05:30 level=INFO source=images.go:458 msg="total blobs: 29"
time=2025-04-24T21:56:56.893+05:30 level=INFO source=images.go:465 msg="total unused blobs removed: 0"
time=2025-04-24T21:56:56.894+05:30 level=INFO source=routes.go:1299 msg="Listening on 127.0.0.1:11434 (version 0.6.6)"
time=2025-04-24T21:56:56.895+05:30 level=DEBUG source=sched.go:107 msg="starting llm scheduler"
time=2025-04-24T21:56:56.950+05:30 level=INFO source=types.go:130 msg="inference compute" id=0 library=metal variant="" compute="" driver=0.0 name="" total="21.3 GiB" available="21.3 GiB"
time=2025-04-24T21:57:13.430+05:30 level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
time=2025-04-24T21:57:13.431+05:30 level=DEBUG source=sched.go:183 msg="updating default concurrency" OLLAMA_MAX_LOADED_MODELS=3 gpu_count=1
time=2025-04-24T21:57:13.435+05:30 level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
time=2025-04-24T21:57:13.440+05:30 level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
time=2025-04-24T21:57:13.440+05:30 level=DEBUG source=sched.go:226 msg="loading first model" model=/Users/sachinsuresh/.ollama/models/blobs/sha256-44d19d212d76a6f3fc442e8411fdb44ea6b67ceccfb00be4b4345c9a4cf813e8
time=2025-04-24T21:57:13.440+05:30 level=DEBUG source=memory.go:108 msg=evaluating library=metal gpu_count=1 available="[21.3 GiB]"
time=2025-04-24T21:57:13.440+05:30 level=WARN source=ggml.go:152 msg="key not found" key=granite.vision.block_count default=0
time=2025-04-24T21:57:13.440+05:30 level=WARN source=ggml.go:152 msg="key not found" key=granite.attention.key_length default=128
time=2025-04-24T21:57:13.440+05:30 level=WARN source=ggml.go:152 msg="key not found" key=granite.attention.value_length default=128
time=2025-04-24T21:57:13.440+05:30 level=INFO source=sched.go:722 msg="new model will fit in available VRAM in single GPU, loading" model=/Users/sachinsuresh/.ollama/models/blobs/sha256-44d19d212d76a6f3fc442e8411fdb44ea6b67ceccfb00be4b4345c9a4cf813e8 gpu=0 parallel=4 available=22906503168 required="13.7 GiB"
time=2025-04-24T21:57:13.441+05:30 level=INFO source=server.go:105 msg="system memory" total="32.0 GiB" free="10.9 GiB" free_swap="0 B"
time=2025-04-24T21:57:13.441+05:30 level=DEBUG source=memory.go:108 msg=evaluating library=metal gpu_count=1 available="[21.3 GiB]"
time=2025-04-24T21:57:13.441+05:30 level=WARN source=ggml.go:152 msg="key not found" key=granite.vision.block_count default=0
time=2025-04-24T21:57:13.442+05:30 level=WARN source=ggml.go:152 msg="key not found" key=granite.attention.key_length default=128
time=2025-04-24T21:57:13.442+05:30 level=WARN source=ggml.go:152 msg="key not found" key=granite.attention.value_length default=128
time=2025-04-24T21:57:13.442+05:30 level=INFO source=server.go:138 msg=offload library=metal layers.requested=-1 layers.model=41 layers.offload=41 layers.split="" memory.available="[21.3 GiB]" memory.gpu_overhead="0 B" memory.required.full="13.7 GiB" memory.required.partial="13.7 GiB" memory.required.kv="5.0 GiB" memory.required.allocations="[13.7 GiB]" memory.weights.total="4.6 GiB" memory.weights.repeating="4.4 GiB" memory.weights.nonrepeating="204.0 MiB" memory.graph.full="3.3 GiB" memory.graph.partial="3.3 GiB"
time=2025-04-24T21:57:13.442+05:30 level=DEBUG source=server.go:262 msg="compatible gpu libraries" compatible=[]
llama_model_load_from_file_impl: using device Metal (Apple M1 Pro) - 21845 MiB free
llama_model_loader: loaded meta data with 40 key-value pairs and 362 tensors from /Users/sachinsuresh/.ollama/models/blobs/sha256-44d19d212d76a6f3fc442e8411fdb44ea6b67ceccfb00be4b4345c9a4cf813e8 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = granite
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Granite 3.1 8b Instruct
llama_model_loader: - kv   3:                           general.finetune str              = instruct
llama_model_loader: - kv   4:                           general.basename str              = granite-3.1
llama_model_loader: - kv   5:                         general.size_label str              = 8B
llama_model_loader: - kv   6:                            general.license str              = apache-2.0
llama_model_loader: - kv   7:                   general.base_model.count u32              = 1
llama_model_loader: - kv   8:                  general.base_model.0.name str              = Granite 3.1 8b Base
llama_model_loader: - kv   9:          general.base_model.0.organization str              = Ibm Granite
llama_model_loader: - kv  10:              general.base_model.0.repo_url str              = https://huggingface.co/ibm-granite/gr...
llama_model_loader: - kv  11:                               general.tags arr[str,3]       = ["language", "granite-3.1", "text-gen...
llama_model_loader: - kv  12:                        granite.block_count u32              = 40
llama_model_loader: - kv  13:                     granite.context_length u32              = 131072
llama_model_loader: - kv  14:                   granite.embedding_length u32              = 4096
llama_model_loader: - kv  15:                granite.feed_forward_length u32              = 12800
llama_model_loader: - kv  16:               granite.attention.head_count u32              = 32
llama_model_loader: - kv  17:            granite.attention.head_count_kv u32              = 8
llama_model_loader: - kv  18:                     granite.rope.freq_base f32              = 10000000.000000
llama_model_loader: - kv  19:   granite.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  20:                          general.file_type u32              = 15
llama_model_loader: - kv  21:                         granite.vocab_size u32              = 49155
llama_model_loader: - kv  22:               granite.rope.dimension_count u32              = 128
llama_model_loader: - kv  23:                    granite.attention.scale f32              = 0.007812
llama_model_loader: - kv  24:                    granite.embedding_scale f32              = 12.000000
llama_model_loader: - kv  25:                     granite.residual_scale f32              = 0.220000
llama_model_loader: - kv  26:                        granite.logit_scale f32              = 16.000000
llama_model_loader: - kv  27:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  28:                         tokenizer.ggml.pre str              = refact
llama_model_loader: - kv  29:                      tokenizer.ggml.tokens arr[str,49155]   = ["<|end_of_text|>", "<fim_prefix>", "...
llama_model_loader: - kv  30:                  tokenizer.ggml.token_type arr[i32,49155]   = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...
llama_model_loader: - kv  31:                      tokenizer.ggml.merges arr[str,48891]   = ["Ġ Ġ", "ĠĠ ĠĠ", "ĠĠĠĠ ĠĠ...
llama_model_loader: - kv  32:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  33:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  34:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  35:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  36:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  37:                    tokenizer.chat_template str              = {%- if messages[0]['role'] == 'system...
llama_model_loader: - kv  38:            tokenizer.ggml.add_space_prefix bool             = false
llama_model_loader: - kv  39:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   81 tensors
llama_model_loader: - type q8_0:    1 tensors
llama_model_loader: - type q4_K:  240 tensors
llama_model_loader: - type q6_K:   40 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.65 GiB (4.89 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:  49152 '<|start_of_role|>' is not marked as EOG
load: control token:      6 '<gh_stars>' is not marked as EOG
load: control token:      0 '<|end_of_text|>' is not marked as EOG
load: control token:  49153 '<|end_of_role|>' is not marked as EOG
load: control token:      3 '<fim_suffix>' is not marked as EOG
load: control token:      1 '<fim_prefix>' is not marked as EOG
load: control token:      9 '<issue_closed>' is not marked as EOG
load: control token:     13 '<jupyter_output>' is not marked as EOG
load: control token:      7 '<issue_start>' is not marked as EOG
load: control token:      5 '<filename>' is not marked as EOG
load: control token:     18 '<reponame>' is not marked as EOG
load: control token:      2 '<fim_middle>' is not marked as EOG
load: control token:     15 '<commit_before>' is not marked as EOG
load: control token:     17 '<commit_after>' is not marked as EOG
load: control token:     14 '<empty_output>' is not marked as EOG
load: control token:     11 '<jupyter_text>' is not marked as EOG
load: control token:      4 '<fim_pad>' is not marked as EOG
load: control token:  49154 '<|tool_call|>' is not marked as EOG
load: control token:      8 '<issue_comment>' is not marked as EOG
load: control token:     12 '<jupyter_code>' is not marked as EOG
load: control token:     10 '<jupyter_start>' is not marked as EOG
load: control token:     16 '<commit_msg>' is not marked as EOG
load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
load: special tokens cache size = 22
load: token to piece cache size = 0.2826 MB
print_info: arch             = granite
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 8.17 B
print_info: general.name     = Granite 3.1 8b Instruct
print_info: f_embedding_scale = 0.000000
print_info: f_residual_scale  = 0.000000
print_info: f_attention_scale = 0.000000
print_info: vocab type       = BPE
print_info: n_vocab          = 49155
print_info: n_merges         = 48891
print_info: BOS token        = 0 '<|end_of_text|>'
print_info: EOS token        = 0 '<|end_of_text|>'
print_info: UNK token        = 0 '<|end_of_text|>'
print_info: PAD token        = 0 '<|end_of_text|>'
print_info: LF token         = 203 'Ċ'
print_info: EOG token        = 0 '<|end_of_text|>'
print_info: max token length = 512
llama_model_load: vocab only - skipping tensors
time=2025-04-24T21:57:13.502+05:30 level=INFO source=server.go:405 msg="starting llama server" cmd="/Applications/Ollama.app/Contents/Resources/ollama runner --model /Users/sachinsuresh/.ollama/models/blobs/sha256-44d19d212d76a6f3fc442e8411fdb44ea6b67ceccfb00be4b4345c9a4cf813e8 --ctx-size 32768 --batch-size 512 --n-gpu-layers 41 --verbose --threads 8 --parallel 4 --port 52368"
time=2025-04-24T21:57:13.502+05:30 level=DEBUG source=server.go:423 msg=subprocess environment="[PATH=/Users/sachinsuresh/Documents/IBM-Code-Assist-Web-UI/path/to/venv/bin:/Users/sachinsuresh/.nvm/versions/node/v22.13.0/bin:/Library/Java/JavaVirtualMachines/jdk1.8.0_311.jdk/Contents/Home/bin:/opt/homebrew/bin:/opt/homebrew/sbin:/usr/local/bin:/System/Cryptexes/App/usr/bin:/usr/bin:/bin:/usr/sbin:/sbin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/local/bin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/bin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/appleinternal/bin:/opt/podman/bin:/Users/sachinsuresh/.nvm/versions/node/v22.13.0/bin:/Library/Java/JavaVirtualMachines/jdk1.8.0_311.jdk/Contents/Home/bin:/Users/sachinsuresh/.local/bin:/Users/sachinsuresh/.vscode/extensions/ms-python.debugpy-2025.6.0-darwin-arm64/bundled/scripts/noConfigScripts:/Users/sachinsuresh/Library/Application Support/Code/User/globalStorage/github.copilot-chat/debugCommand:/Users/sachinsuresh/.local/bin:/Users/sachinsuresh/.local/bin DYLD_LIBRARY_PATH=/Applications/Ollama.app/Contents/Resources]"
time=2025-04-24T21:57:13.504+05:30 level=INFO source=sched.go:451 msg="loaded runners" count=1
time=2025-04-24T21:57:13.505+05:30 level=INFO source=server.go:580 msg="waiting for llama runner to start responding"
time=2025-04-24T21:57:13.505+05:30 level=INFO source=server.go:614 msg="waiting for server to become available" status="llm server error"
time=2025-04-24T21:57:13.518+05:30 level=INFO source=runner.go:853 msg="starting go runner"
time=2025-04-24T21:57:13.518+05:30 level=DEBUG source=ggml.go:99 msg="ggml backend load all from path" path=/Applications/Ollama.app/Contents/Resources
time=2025-04-24T21:57:13.522+05:30 level=INFO source=ggml.go:109 msg=system Metal.0.EMBED_LIBRARY=1 CPU.0.NEON=1 CPU.0.ARM_FMA=1 CPU.0.FP16_VA=1 CPU.0.DOTPROD=1 CPU.0.LLAMAFILE=1 CPU.0.ACCELERATE=1 compiler=cgo(clang)
time=2025-04-24T21:57:13.522+05:30 level=INFO source=runner.go:913 msg="Server listening on 127.0.0.1:52368"
llama_model_load_from_file_impl: using device Metal (Apple M1 Pro) - 21845 MiB free
llama_model_loader: loaded meta data with 40 key-value pairs and 362 tensors from /Users/sachinsuresh/.ollama/models/blobs/sha256-44d19d212d76a6f3fc442e8411fdb44ea6b67ceccfb00be4b4345c9a4cf813e8 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = granite
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Granite 3.1 8b Instruct
llama_model_loader: - kv   3:                           general.finetune str              = instruct
llama_model_loader: - kv   4:                           general.basename str              = granite-3.1
llama_model_loader: - kv   5:                         general.size_label str              = 8B
llama_model_loader: - kv   6:                            general.license str              = apache-2.0
llama_model_loader: - kv   7:                   general.base_model.count u32              = 1
llama_model_loader: - kv   8:                  general.base_model.0.name str              = Granite 3.1 8b Base
llama_model_loader: - kv   9:          general.base_model.0.organization str              = Ibm Granite
llama_model_loader: - kv  10:              general.base_model.0.repo_url str              = https://huggingface.co/ibm-granite/gr...
llama_model_loader: - kv  11:                               general.tags arr[str,3]       = ["language", "granite-3.1", "text-gen...
llama_model_loader: - kv  12:                        granite.block_count u32              = 40
llama_model_loader: - kv  13:                     granite.context_length u32              = 131072
llama_model_loader: - kv  14:                   granite.embedding_length u32              = 4096
llama_model_loader: - kv  15:                granite.feed_forward_length u32              = 12800
llama_model_loader: - kv  16:               granite.attention.head_count u32              = 32
llama_model_loader: - kv  17:            granite.attention.head_count_kv u32              = 8
llama_model_loader: - kv  18:                     granite.rope.freq_base f32              = 10000000.000000
llama_model_loader: - kv  19:   granite.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  20:                          general.file_type u32              = 15
llama_model_loader: - kv  21:                         granite.vocab_size u32              = 49155
llama_model_loader: - kv  22:               granite.rope.dimension_count u32              = 128
llama_model_loader: - kv  23:                    granite.attention.scale f32              = 0.007812
llama_model_loader: - kv  24:                    granite.embedding_scale f32              = 12.000000
llama_model_loader: - kv  25:                     granite.residual_scale f32              = 0.220000
llama_model_loader: - kv  26:                        granite.logit_scale f32              = 16.000000
llama_model_loader: - kv  27:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  28:                         tokenizer.ggml.pre str              = refact
llama_model_loader: - kv  29:                      tokenizer.ggml.tokens arr[str,49155]   = ["<|end_of_text|>", "<fim_prefix>", "...
llama_model_loader: - kv  30:                  tokenizer.ggml.token_type arr[i32,49155]   = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...
llama_model_loader: - kv  31:                      tokenizer.ggml.merges arr[str,48891]   = ["Ġ Ġ", "ĠĠ ĠĠ", "ĠĠĠĠ ĠĠ...
llama_model_loader: - kv  32:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  33:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  34:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  35:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  36:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  37:                    tokenizer.chat_template str              = {%- if messages[0]['role'] == 'system...
llama_model_loader: - kv  38:            tokenizer.ggml.add_space_prefix bool             = false
llama_model_loader: - kv  39:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   81 tensors
llama_model_loader: - type q8_0:    1 tensors
llama_model_loader: - type q4_K:  240 tensors
llama_model_loader: - type q6_K:   40 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.65 GiB (4.89 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:  49152 '<|start_of_role|>' is not marked as EOG
load: control token:      6 '<gh_stars>' is not marked as EOG
load: control token:      0 '<|end_of_text|>' is not marked as EOG
load: control token:  49153 '<|end_of_role|>' is not marked as EOG
load: control token:      3 '<fim_suffix>' is not marked as EOG
load: control token:      1 '<fim_prefix>' is not marked as EOG
load: control token:      9 '<issue_closed>' is not marked as EOG
load: control token:     13 '<jupyter_output>' is not marked as EOG
load: control token:      7 '<issue_start>' is not marked as EOG
load: control token:      5 '<filename>' is not marked as EOG
load: control token:     18 '<reponame>' is not marked as EOG
load: control token:      2 '<fim_middle>' is not marked as EOG
load: control token:     15 '<commit_before>' is not marked as EOG
load: control token:     17 '<commit_after>' is not marked as EOG
load: control token:     14 '<empty_output>' is not marked as EOG
load: control token:     11 '<jupyter_text>' is not marked as EOG
load: control token:      4 '<fim_pad>' is not marked as EOG
load: control token:  49154 '<|tool_call|>' is not marked as EOG
load: control token:      8 '<issue_comment>' is not marked as EOG
load: control token:     12 '<jupyter_code>' is not marked as EOG
load: control token:     10 '<jupyter_start>' is not marked as EOG
load: control token:     16 '<commit_msg>' is not marked as EOG
load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
load: special tokens cache size = 22
load: token to piece cache size = 0.2826 MB
print_info: arch             = granite
print_info: vocab_only       = 0
print_info: n_ctx_train      = 131072
print_info: n_embd           = 4096
print_info: n_layer          = 40
print_info: n_head           = 32
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 4
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 1.6e+01
print_info: f_attn_scale     = 7.8e-03
print_info: n_ff             = 12800
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 0
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 131072
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 3B
print_info: model params     = 8.17 B
print_info: general.name     = Granite 3.1 8b Instruct
print_info: f_embedding_scale = 12.000000
print_info: f_residual_scale  = 0.220000
print_info: f_attention_scale = 0.007812
print_info: vocab type       = BPE
print_info: n_vocab          = 49155
print_info: n_merges         = 48891
print_info: BOS token        = 0 '<|end_of_text|>'
print_info: EOS token        = 0 '<|end_of_text|>'
print_info: UNK token        = 0 '<|end_of_text|>'
print_info: PAD token        = 0 '<|end_of_text|>'
print_info: LF token         = 203 'Ċ'
print_info: EOG token        = 0 '<|end_of_text|>'
print_info: max token length = 512
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal, is_swa = 0
load_tensors: layer   1 assigned to device Metal, is_swa = 0
load_tensors: layer   2 assigned to device Metal, is_swa = 0
load_tensors: layer   3 assigned to device Metal, is_swa = 0
load_tensors: layer   4 assigned to device Metal, is_swa = 0
load_tensors: layer   5 assigned to device Metal, is_swa = 0
load_tensors: layer   6 assigned to device Metal, is_swa = 0
load_tensors: layer   7 assigned to device Metal, is_swa = 0
load_tensors: layer   8 assigned to device Metal, is_swa = 0
load_tensors: layer   9 assigned to device Metal, is_swa = 0
load_tensors: layer  10 assigned to device Metal, is_swa = 0
load_tensors: layer  11 assigned to device Metal, is_swa = 0
load_tensors: layer  12 assigned to device Metal, is_swa = 0
load_tensors: layer  13 assigned to device Metal, is_swa = 0
load_tensors: layer  14 assigned to device Metal, is_swa = 0
load_tensors: layer  15 assigned to device Metal, is_swa = 0
load_tensors: layer  16 assigned to device Metal, is_swa = 0
load_tensors: layer  17 assigned to device Metal, is_swa = 0
load_tensors: layer  18 assigned to device Metal, is_swa = 0
load_tensors: layer  19 assigned to device Metal, is_swa = 0
load_tensors: layer  20 assigned to device Metal, is_swa = 0
load_tensors: layer  21 assigned to device Metal, is_swa = 0
load_tensors: layer  22 assigned to device Metal, is_swa = 0
load_tensors: layer  23 assigned to device Metal, is_swa = 0
load_tensors: layer  24 assigned to device Metal, is_swa = 0
load_tensors: layer  25 assigned to device Metal, is_swa = 0
load_tensors: layer  26 assigned to device Metal, is_swa = 0
load_tensors: layer  27 assigned to device Metal, is_swa = 0
load_tensors: layer  28 assigned to device Metal, is_swa = 0
load_tensors: layer  29 assigned to device Metal, is_swa = 0
load_tensors: layer  30 assigned to device Metal, is_swa = 0
load_tensors: layer  31 assigned to device Metal, is_swa = 0
load_tensors: layer  32 assigned to device Metal, is_swa = 0
load_tensors: layer  33 assigned to device Metal, is_swa = 0
load_tensors: layer  34 assigned to device Metal, is_swa = 0
load_tensors: layer  35 assigned to device Metal, is_swa = 0
load_tensors: layer  36 assigned to device Metal, is_swa = 0
load_tensors: layer  37 assigned to device Metal, is_swa = 0
load_tensors: layer  38 assigned to device Metal, is_swa = 0
load_tensors: layer  39 assigned to device Metal, is_swa = 0
load_tensors: layer  40 assigned to device Metal, is_swa = 0
ggml_backend_metal_log_allocated_size: allocated buffer, size =  4758.73 MiB, ( 4758.80 / 21845.34)
load_tensors: offloading 40 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 41/41 layers to GPU
load_tensors:   CPU_Mapped model buffer size =   204.01 MiB
load_tensors: Metal_Mapped model buffer size =  4758.72 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 4
llama_context: n_ctx         = 32768
llama_context: n_ctx_per_seq = 8192
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 10000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M1 Pro
ggml_metal_init: picking default device: Apple M1 Pro
ggml_metal_load_library: using embedded metal library
ggml_metal_init: GPU name:   Apple M1 Pro
ggml_metal_init: GPU family: MTLGPUFamilyApple7  (1007)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = false
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = false
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 22906.50 MB
ggml_metal_init: loaded kernel_add                                    0x12e9f12a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12e9f04c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12e9f1990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12e9f31e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12e9f40e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12e9f23c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12e9f4520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12e9f4c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12e9f4e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12e9f5ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12e9f6360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12e9f6b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12e9f77d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12e9f8a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12e9f9580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12e9f9fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12e9fa9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12e9fac70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12e9fb2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12e8c1b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12e8c2570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12e8c1f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12e8c3a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12e8c14e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12e8c5040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12e9fbdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12e9fc260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12e9fd2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12e9fda80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12e9fe1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12e9fe9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12e9ff150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12e9ff5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12e9ffda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12d709880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12d70f4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12d70f8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x128d04d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x128d060e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x128d06620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x128d05a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x128d07ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x128d08680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x128d08e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x128d096a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12912b9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12912c690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12912d970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12912ca90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12912d000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12912d270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12d104370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12d104760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12d1056c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_l2_norm                                0x12d105e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12d106680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12d106e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12d1075a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12d107db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rwkv_wkv6_f32                          0x12d108460 | th_max =  448 | th_width =   32
ggml_metal_init: loaded kernel_rwkv_wkv7_f32                          0x12d108c60 | th_max =  448 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12d109440 | th_max = 1024 | th_width =   32
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12d109c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12d10a470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12d10aca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12d10aed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12d10b680 | th_max =  640 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12912f5a0 | th_max =  832 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12912fdf0 | th_max =  640 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x129130b70 | th_max =  576 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x129130280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x129131b00 | th_max =  896 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12e8c4190 | th_max =  640 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12e8c4840 | th_max =  512 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12e8c59e0 | th_max =  576 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12e8c6780 | th_max =  832 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12d10be40 | th_max =  704 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12d10c230 | th_max =  640 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12d10c620 | th_max =  640 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x129131440 | th_max =  768 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x1291324e0 | th_max =  704 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x129133370 | th_max =  640 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x129133bc0 | th_max =  640 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x129134400 | th_max =  640 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x129134c40 | th_max =  704 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12d10cea0 | th_max =  640 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12d10d690 | th_max =  640 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12d10e590 | th_max =  640 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12d10ee00 | th_max =  704 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12d10f610 | th_max =  640 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12d10fe60 | th_max =  640 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12d110670 | th_max =  832 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12d110eb0 | th_max =  704 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12d1116f0 | th_max =  640 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12d111f30 | th_max =  640 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12d112770 | th_max =  832 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12d112fb0 | th_max =  704 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12d1137f0 | th_max =  640 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12d114030 | th_max =  640 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12d114870 | th_max =  704 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12d1150b0 | th_max =  704 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12d1158f0 | th_max =  640 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12d116130 | th_max =  640 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12d116970 | th_max =  768 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12d1171b0 | th_max =  704 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12d1179f0 | th_max =  640 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12d118230 | th_max =  640 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12d118a70 | th_max =  832 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12d1192b0 | th_max =  704 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12d119af0 | th_max =  640 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12d11a330 | th_max =  640 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12d11ab70 | th_max =  640 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12d11b390 | th_max =  576 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12d10da80 | th_max =  576 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12d11c2f0 | th_max =  576 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12d11bc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12d11d2f0 | th_max =  832 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12d11dad0 | th_max =  704 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12d11e2f0 | th_max =  768 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12d11eb00 | th_max =  640 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12d11f310 | th_max =  704 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12d11fb20 | th_max =  448 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12d120620 | th_max =  576 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12d120e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12d121680 | th_max =  896 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12d121ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12d122700 | th_max = 1024 | th_width =   32
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12d122f60 | th_max =  832 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12d1237a0 | th_max =  832 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12d123fe0 | th_max =  576 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12d124820 | th_max =  576 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12d125060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12d1258a0 | th_max =  576 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12d1260e0 | th_max =  576 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12d61c5a0 | th_max =  576 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12d61cc90 | th_max =  576 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12d61d090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12d61e0b0 | th_max =  832 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12d61efa0 | th_max =  640 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12d61d4a0 | th_max =  704 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12d61f9a0 | th_max =  640 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12d620200 | th_max =  640 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12d620a40 | th_max =  448 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12d11cc20 | th_max =  576 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12d126ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12d1279b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12d128210 | th_max =  832 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12d128990 | th_max =  832 | th_width =   32
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12d129180 | th_max =  768 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12d1299a0 | th_max =  768 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x128f080b0 | th_max =  768 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x128f06c20 | th_max =  768 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x128f06ff0 | th_max =  832 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x128f0a6c0 | th_max =  832 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x128f09e60 | th_max =  832 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x128f0bcd0 | th_max =  832 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x128f0b5c0 | th_max =  768 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12e8c6050 | th_max =  832 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12e8c7160 | th_max =  768 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12e8c8010 | th_max =  832 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12d127230 | th_max =  832 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12d12a310 | th_max =  832 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12d12a700 | th_max =  832 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12d12b990 | th_max =  832 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12e8c78d0 | th_max =  832 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12e8c89e0 | th_max =  832 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12e8c98c0 | th_max =  832 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12e8ca0a0 | th_max =  832 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12e8ca900 | th_max =  832 | th_width =   32
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f32                (not supported)
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12e8cb140 | th_max =  832 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12e8cb9a0 | th_max =  832 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12e8c91a0 | th_max =  768 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12e8cca40 | th_max =  768 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12e8cd280 | th_max =  832 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12e8cd4b0 | th_max =  832 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12e8cdca0 | th_max =  832 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12d12b270 | th_max =  832 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12d12c3a0 | th_max =  832 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12d12d280 | th_max =  832 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12d12dae0 | th_max =  832 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12d12e320 | th_max =  832 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12d12eb80 | th_max =  832 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12d12f3c0 | th_max =  832 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12d12fc20 | th_max =  832 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12d70fdf0 | th_max =  832 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12d711860 | th_max =  832 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12d710920 | th_max =  832 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12d7128e0 | th_max =  832 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12d130670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12d130a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12d130f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12d131b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12d132370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12d132b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12d133330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12d133b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12d1342f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12d134b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12d61e730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12d621940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12d622170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_unpad_f32                              0x12d623090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12d622a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12d1313f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12d1353b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12d1357a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12d1364f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12d1359d0 | th_max =  640 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12d135c60 | th_max =  640 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12d138180 | th_max =  640 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12d1389e0 | th_max =  576 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12d1398b0 | th_max =  576 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h192                0x12d139ae0 | th_max =  576 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_hk192_hv128         0x12d6239b0 | th_max =  576 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12d6240a0 | th_max =  832 | th_width =   32
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12d624490 | th_max =  640 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x128d09fd0 | th_max =  768 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12d6d0390 | th_max =  768 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12d624ba0 | th_max =  768 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x128d0a7b0 | th_max =  704 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h192               0x128d0aba0 | th_max =  704 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_hk192_hv128        0x129135890 | th_max =  704 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x128d0b150 | th_max =  704 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12d13aab0 | th_max =  640 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12d13aea0 | th_max =  768 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x129135dd0 | th_max =  768 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x1291361c0 | th_max =  768 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12d13be60 | th_max =  704 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h192               0x12d13c090 | th_max =  704 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_hk192_hv128        0x12d13caa0 | th_max =  704 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12d13c810 | th_max =  704 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12d13b720 | th_max =  576 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12e8ce4e0 | th_max =  640 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12e8ced40 | th_max =  640 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12e8cf540 | th_max =  640 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12d13dd40 | th_max =  640 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h192               0x12d13e580 | th_max =  640 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_hk192_hv128        0x12d712200 | th_max =  640 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12d7132b0 | th_max =  640 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12d13f700 | th_max =  576 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12d13faf0 | th_max =  640 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12d1404d0 | th_max =  640 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12d140090 | th_max =  640 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12d140de0 | th_max =  640 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h192               0x12d141e80 | th_max =  640 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_hk192_hv128        0x12d1426e0 | th_max =  640 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12d142f70 | th_max =  640 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12d1437d0 | th_max =  640 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12d144000 | th_max =  768 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12d144830 | th_max =  768 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12d145090 | th_max =  768 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12d714090 | th_max =  768 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h192               0x12d7095c0 | th_max =  768 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_hk192_hv128        0x12d145a50 | th_max =  768 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12d145e40 | th_max =  768 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12d146df0 | th_max = 1024 | th_width =   32
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12d147650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12d146760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12d148080 | th_max =  832 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12d1488d0 | th_max =  832 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12d149140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h192            0x12d149990 | th_max =  832 | th_width =   32
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h192           0x12d14a1f0 | th_max =  896 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h192           0x12d14aa50 | th_max =  896 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h192           0x12d14b2b0 | th_max =  768 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h192           0x12d14bb10 | th_max =  704 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h192           0x12d14c370 | th_max =  896 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_hk192_hv128      0x12d14cbd0 | th_max =  896 | th_width =   32
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_hk192_hv128      0x12d14d430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_hk192_hv128      0x12d14dca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_hk192_hv128      0x12d14e500 | th_max =  832 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_hk192_hv128      0x12d14ed60 | th_max =  768 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_hk192_hv128      0x12d14f5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12d14fe20 | th_max =  768 | th_width =   32
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12d1506b0 | th_max =  832 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12d150ef0 | th_max =  768 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12d151750 | th_max =  640 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12d714480 | th_max =  576 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x128f0cd80 | th_max =  768 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x128d0bdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x129136ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x129137150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x1291379b0 | th_max = 1024 | th_width =   32
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12e8cf940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12e8cfd50 | th_max = 1024 | th_width =   32
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12e8d05a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12e8d1510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12e8d1cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12e8d2bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12d146230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12d141830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x12d1533a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x12d152d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x129138920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x129138d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x12d154220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x12d1548b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x12e8d2460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x12e8d3530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x12e8d3920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x12d154b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12d155400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12d1557f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12d1575b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12d158030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12d158a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_neg                                    0x12d1594b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12d158e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12d159c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12d15a460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12d15acb0 | th_max = 1024 | th_width =   32
set_abort_callback: call
llama_context:        CPU  output buffer size =     0.81 MiB
llama_context: n_ctx = 32768
llama_context: n_ctx = 32768 (padded)
init: kv_size = 32768, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 40, can_shift = 1
init: layer   0: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024, dev = Metal
init: layer   1: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024, dev = Metal
init: layer   2: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024, dev = Metal
init: layer   3: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024, dev = Metal
init: layer   4: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024, dev = Metal
init: layer   5: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024, dev = Metal
init: layer   6: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024, dev = Metal
init: layer   7: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024, dev = Metal
init: layer   8: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024, dev = Metal
init: layer   9: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024, dev = Metal
init: layer  10: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024, dev = Metal
init: layer  11: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024, dev = Metal
init: layer  12: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024, dev = Metal
init: layer  13: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024, dev = Metal
init: layer  14: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024, dev = Metal
init: layer  15: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024, dev = Metal
init: layer  16: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024, dev = Metal
init: layer  17: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024, dev = Metal
init: layer  18: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024, dev = Metal
init: layer  19: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024, dev = Metal
init: layer  20: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024, dev = Metal
init: layer  21: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024, dev = Metal
init: layer  22: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024, dev = Metal
init: layer  23: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024, dev = Metal
init: layer  24: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024, dev = Metal
init: layer  25: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024, dev = Metal
init: layer  26: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024, dev = Metal
init: layer  27: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024, dev = Metal
init: layer  28: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024, dev = Metal
init: layer  29: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024, dev = Metal
init: layer  30: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024, dev = Metal
init: layer  31: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024, dev = Metal
init: layer  32: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024, dev = Metal
init: layer  33: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024, dev = Metal
init: layer  34: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024, dev = Metal
init: layer  35: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024, dev = Metal
init: layer  36: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024, dev = Metal
init: layer  37: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024, dev = Metal
init: layer  38: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024, dev = Metal
init: layer  39: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024, dev = Metal
time=2025-04-24T21:57:13.757+05:30 level=INFO source=server.go:614 msg="waiting for server to become available" status="llm server loading model"
time=2025-04-24T21:57:13.757+05:30 level=DEBUG source=server.go:625 msg="model load progress 1.00"
time=2025-04-24T21:57:14.008+05:30 level=DEBUG source=server.go:628 msg="model load completed, waiting for server to become available" status="llm server loading model"
init:      Metal KV buffer size =  5120.00 MiB
llama_context: KV self size  = 5120.00 MiB, K (f16): 2560.00 MiB, V (f16): 2560.00 MiB
llama_context: enumerating backends
llama_context: backend_ptrs.size() = 2
llama_context: max_nodes = 65536
llama_context: worst-case: n_tokens = 512, n_seqs = 1, n_outputs = 0
llama_context: reserving graph for n_tokens = 512, n_seqs = 1
llama_context: reserving graph for n_tokens = 1, n_seqs = 1
llama_context: reserving graph for n_tokens = 512, n_seqs = 1
llama_context:      Metal compute buffer size =  2144.00 MiB
llama_context:        CPU compute buffer size =    72.01 MiB
llama_context: graph nodes  = 1448
llama_context: graph splits = 2
time=2025-04-24T21:57:14.258+05:30 level=INFO source=server.go:619 msg="llama runner started in 0.75 seconds"
time=2025-04-24T21:57:14.259+05:30 level=DEBUG source=sched.go:464 msg="finished setting up runner" model=/Users/sachinsuresh/.ollama/models/blobs/sha256-44d19d212d76a6f3fc442e8411fdb44ea6b67ceccfb00be4b4345c9a4cf813e8
time=2025-04-24T21:57:14.261+05:30 level=DEBUG source=routes.go:1523 msg="chat request" images=0 prompt="<|start_of_role|>system<|end_of_role|><important_rules>\n  Always include the language and file name in the info string when you write code blocks. If you are editing \"src/main.py\" for example, your code block should start with '```python src/main.py'.\n</important_rules><|end_of_text|> You are a helpful AI assistant.<|end_of_text|>\n<|start_of_role|>user<|end_of_role|>answer briefWS?<|end_of_text|>\n<|start_of_role|>assistant<|end_of_role|>"
time=2025-04-24T21:57:14.263+05:30 level=DEBUG source=cache.go:104 msg="loading cache slot" id=0 cache=0 prompt=83 used=0 remaining=83
[GIN] 2025/04/24 - 21:57:35 | 200 | 22.358321458s |       127.0.0.1 | POST     "/api/chat"
time=2025-04-24T21:57:35.780+05:30 level=DEBUG source=sched.go:468 msg="context for request finished"
time=2025-04-24T21:57:35.780+05:30 level=DEBUG source=sched.go:341 msg="runner with non-zero duration has gone idle, adding timer" modelPath=/Users/sachinsuresh/.ollama/models/blobs/sha256-44d19d212d76a6f3fc442e8411fdb44ea6b67ceccfb00be4b4345c9a4cf813e8 duration=30m0s
time=2025-04-24T21:57:35.780+05:30 level=DEBUG source=sched.go:359 msg="after processing request finished event" modelPath=/Users/sachinsuresh/.ollama/models/blobs/sha256-44d19d212d76a6f3fc442e8411fdb44ea6b67ceccfb00be4b4345c9a4cf813e8 refCount=0
time=2025-04-24T21:57:35.837+05:30 level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
time=2025-04-24T21:57:35.838+05:30 level=DEBUG source=sched.go:577 msg="evaluating already loaded" model=/Users/sachinsuresh/.ollama/models/blobs/sha256-44d19d212d76a6f3fc442e8411fdb44ea6b67ceccfb00be4b4345c9a4cf813e8
time=2025-04-24T21:57:35.841+05:30 level=DEBUG source=routes.go:1523 msg="chat request" images=0 prompt="<|start_of_role|>system<|end_of_role|>Knowledge Cutoff Date: April 2024.\nYou are Granite, developed by IBM.<|end_of_text|> You are a helpful AI assistant.<|end_of_text|>\n<|start_of_role|>user<|end_of_role|>Given the following... please reply with a title for the chat that is 3-4 words in length, all words used should be directly related to the content of the chat, avoid using verbs unless they are directly related to the content of the chat, no additional text or explanation, you don't need ending punctuation.\n\nIt seems like there's a typo in your message. If you're asking about \"bfs\" or \"DFS\", these are common abbreviations used in computer science:\n\n1. BFS stands for Breadth-First Search, an algorithm used to traverse or search tree or graph data structures. It explores all the neighboring nodes at the present depth level before moving on to nodes at the next depth level.\n\n2. DFS stands for Depth-First Search, another algorithm used for traversing or searching a graph or tree data structure. Unlike BFS, DFS explores as far as possible along each branch before backtracking.\n\nHere's an example of both in Python:\n\n\n\nRemember to replace `graph` and `root`/`node` with your actual data structure.<|end_of_text|>\n<|start_of_role|>assistant<|end_of_role|>"
time=2025-04-24T21:57:35.845+05:30 level=DEBUG source=cache.go:104 msg="loading cache slot" id=0 cache=381 prompt=274 used=3 remaining=271
[GIN] 2025/04/24 - 21:57:38 | 200 |  2.210616166s |       127.0.0.1 | POST     "/api/chat"
time=2025-04-24T21:57:38.037+05:30 level=DEBUG source=sched.go:409 msg="context for request finished"
time=2025-04-24T21:57:38.037+05:30 level=DEBUG source=sched.go:341 msg="runner with non-zero duration has gone idle, adding timer" modelPath=/Users/sachinsuresh/.ollama/models/blobs/sha256-44d19d212d76a6f3fc442e8411fdb44ea6b67ceccfb00be4b4345c9a4cf813e8 duration=30m0s
time=2025-04-24T21:57:38.037+05:30 level=DEBUG source=sched.go:359 msg="after processing request finished event" modelPath=/Users/sachinsuresh/.ollama/models/blobs/sha256-44d19d212d76a6f3fc442e8411fdb44ea6b67ceccfb00be4b4345c9a4cf813e8 refCount=0
time=2025-04-24T21:58:04.371+05:30 level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
time=2025-04-24T21:58:04.371+05:30 level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
time=2025-04-24T21:58:04.371+05:30 level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
time=2025-04-24T21:58:04.371+05:30 level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
time=2025-04-24T21:58:04.386+05:30 level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
time=2025-04-24T21:58:04.387+05:30 level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
time=2025-04-24T21:58:04.387+05:30 level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
time=2025-04-24T21:58:04.387+05:30 level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/04/24 - 21:58:04 | 200 |   48.184583ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/04/24 - 21:58:04 | 200 |   48.560459ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/04/24 - 21:58:04 | 200 |   40.914958ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/04/24 - 21:58:04 | 200 |   38.428292ms |       127.0.0.1 | POST     "/api/show"
time=2025-04-24T21:58:04.567+05:30 level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
time=2025-04-24T21:58:04.580+05:30 level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/04/24 - 21:58:04 | 200 |   30.571042ms |       127.0.0.1 | POST     "/api/show"
time=2025-04-24T21:58:08.144+05:30 level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
time=2025-04-24T21:58:08.160+05:30 level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/04/24 - 21:58:08 | 200 |   38.078167ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/04/24 - 21:58:13 | 200 |      37.541µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/04/24 - 21:58:13 | 200 |      73.917µs |       127.0.0.1 | GET      "/api/ps"
time=2025-04-24T21:58:18.536+05:30 level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
time=2025-04-24T21:58:18.537+05:30 level=DEBUG source=sched.go:577 msg="evaluating already loaded" model=/Users/sachinsuresh/.ollama/models/blobs/sha256-44d19d212d76a6f3fc442e8411fdb44ea6b67ceccfb00be4b4345c9a4cf813e8
time=2025-04-24T21:58:18.540+05:30 level=DEBUG source=routes.go:1523 msg="chat request" images=0 prompt="<|start_of_role|>system<|end_of_role|><important_rules>\n  Always include the language and file name in the info string when you write code blocks. If you are editing \"src/main.py\" for example, your code block should start with '```python src/main.py'.\n</important_rules><|end_of_text|> You are a helpful AI assistant.<|end_of_text|>\n<|start_of_role|>user<|end_of_role|>answer briefWS?<|end_of_text|>\n<|start_of_role|>assistant<|end_of_role|>It seems like there's a typo in your message. If you're asking about \"bfs\" or \"DFS\", these are common abbreviations used in computer science:\n\n1. BFS stands for Breadth-First Search, an algorithm used to traverse or search tree or graph data structures. It explores all the neighboring nodes at the present depth level before moving on to nodes at the next depth level.\n\n2. DFS stands for Depth-First Search, another algorithm used for traversing or searching a graph or tree data structure. Unlike BFS, DFS explores as far as possible along each branch before backtracking.\n\nHere's an example of both in Python:\n\n```python src/main.py\nfrom collections import deque\n\ndef bfs(graph, root):\n    visited = set()\n    queue = deque([root])\n\n    while queue:\n        vertex = queue.popleft()\n        if vertex not in visited:\n            visited.add(vertex)\n            queue.extend(graph[vertex] - visited)\n\n    return visited\n\ndef dfs(graph, node, visited=None):\n    if visited is None:\n        visited = set()\n\n    visited.add(node)\n    for neighbour in graph[node] - visited:\n        dfs(graph, neighbour, visited)\n\n    return visited\n```\n\nRemember to replace `graph` and `root`/`node` with your actual data structure.<|end_of_text|>\n<|start_of_role|>user<|end_of_role|>What is java?<|end_of_text|>\n<|start_of_role|>assistant<|end_of_role|>"
time=2025-04-24T21:58:18.542+05:30 level=DEBUG source=cache.go:104 msg="loading cache slot" id=0 cache=284 prompt=393 used=3 remaining=390
[GIN] 2025/04/24 - 21:58:33 | 200 | 15.361974583s |       127.0.0.1 | POST     "/api/chat"
time=2025-04-24T21:58:33.890+05:30 level=DEBUG source=sched.go:409 msg="context for request finished"
time=2025-04-24T21:58:33.891+05:30 level=DEBUG source=sched.go:341 msg="runner with non-zero duration has gone idle, adding timer" modelPath=/Users/sachinsuresh/.ollama/models/blobs/sha256-44d19d212d76a6f3fc442e8411fdb44ea6b67ceccfb00be4b4345c9a4cf813e8 duration=30m0s
time=2025-04-24T21:58:33.891+05:30 level=DEBUG source=sched.go:359 msg="after processing request finished event" modelPath=/Users/sachinsuresh/.ollama/models/blobs/sha256-44d19d212d76a6f3fc442e8411fdb44ea6b67ceccfb00be4b4345c9a4cf813e8 refCount=0
time=2025-04-24T22:15:20.013+05:30 level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
time=2025-04-24T22:15:20.013+05:30 level=DEBUG source=sched.go:577 msg="evaluating already loaded" model=/Users/sachinsuresh/.ollama/models/blobs/sha256-44d19d212d76a6f3fc442e8411fdb44ea6b67ceccfb00be4b4345c9a4cf813e8
time=2025-04-24T22:15:20.017+05:30 level=DEBUG source=routes.go:1523 msg="chat request" images=0 prompt="<|start_of_role|>system<|end_of_role|><important_rules>\n  Always include the language and file name in the info string when you write code blocks. If you are editing \"src/main.py\" for example, your code block should start with '```python src/main.py'.\n</important_rules><|end_of_text|> You are a helpful AI assistant.<|end_of_text|>\n<|start_of_role|>user<|end_of_role|>hi granite<|end_of_text|>\n<|start_of_role|>assistant<|end_of_role|>"
time=2025-04-24T22:15:20.020+05:30 level=DEBUG source=cache.go:104 msg="loading cache slot" id=0 cache=610 prompt=82 used=74 remaining=8
[GIN] 2025/04/24 - 22:15:35 | 200 | 15.825604958s |       127.0.0.1 | POST     "/api/chat"
time=2025-04-24T22:15:35.830+05:30 level=DEBUG source=sched.go:409 msg="context for request finished"
time=2025-04-24T22:15:35.831+05:30 level=DEBUG source=sched.go:341 msg="runner with non-zero duration has gone idle, adding timer" modelPath=/Users/sachinsuresh/.ollama/models/blobs/sha256-44d19d212d76a6f3fc442e8411fdb44ea6b67ceccfb00be4b4345c9a4cf813e8 duration=30m0s
time=2025-04-24T22:15:35.831+05:30 level=DEBUG source=sched.go:359 msg="after processing request finished event" modelPath=/Users/sachinsuresh/.ollama/models/blobs/sha256-44d19d212d76a6f3fc442e8411fdb44ea6b67ceccfb00be4b4345c9a4cf813e8 refCount=0
time=2025-04-24T22:15:35.868+05:30 level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
time=2025-04-24T22:15:35.869+05:30 level=DEBUG source=sched.go:577 msg="evaluating already loaded" model=/Users/sachinsuresh/.ollama/models/blobs/sha256-44d19d212d76a6f3fc442e8411fdb44ea6b67ceccfb00be4b4345c9a4cf813e8
time=2025-04-24T22:15:35.872+05:30 level=DEBUG source=routes.go:1523 msg="chat request" images=0 prompt="<|start_of_role|>system<|end_of_role|>Knowledge Cutoff Date: April 2024.\nYou are Granite, developed by IBM.<|end_of_text|> You are a helpful AI assistant.<|end_of_text|>\n<|start_of_role|>user<|end_of_role|>Given the following... please reply with a title for the chat that is 3-4 words in length, all words used should be directly related to the content of the chat, avoid using verbs unless they are directly related to the content of the chat, no additional text or explanation, you don't need ending punctuation.\n\nHello! I'm here to assist you. How can I help you today? If you have any questions or need assistance with Python code in \"src/main.py\", feel free to ask. Here's an example of how our interaction might look:\n\nUser: Hi, I need help understanding this piece of code in 'src/main.py'. \n\n\n\nAssistant: Of course! This Python script defines a function called `greet` that takes one parameter, `name`. The function returns a string that concatenates the word \"Hello,\" with a space and then the value of the parameter `name`. \n\nThe last line calls this function with the argument \"Alice\", which prints the result to the console. When you run this script, it should output: \"Hello, Alice!\"<|end_of_text|>\n<|start_of_role|>assistant<|end_of_role|>"
time=2025-04-24T22:15:35.876+05:30 level=DEBUG source=cache.go:104 msg="loading cache slot" id=0 cache=281 prompt=273 used=3 remaining=270
[GIN] 2025/04/24 - 22:15:38 | 200 |   2.29377875s |       127.0.0.1 | POST     "/api/chat"
time=2025-04-24T22:15:38.150+05:30 level=DEBUG source=sched.go:409 msg="context for request finished"
time=2025-04-24T22:15:38.150+05:30 level=DEBUG source=sched.go:341 msg="runner with non-zero duration has gone idle, adding timer" modelPath=/Users/sachinsuresh/.ollama/models/blobs/sha256-44d19d212d76a6f3fc442e8411fdb44ea6b67ceccfb00be4b4345c9a4cf813e8 duration=30m0s
time=2025-04-24T22:15:38.150+05:30 level=DEBUG source=sched.go:359 msg="after processing request finished event" modelPath=/Users/sachinsuresh/.ollama/models/blobs/sha256-44d19d212d76a6f3fc442e8411fdb44ea6b67ceccfb00be4b4345c9a4cf813e8 refCount=0
