2025/04/25 09:31:34 routes.go:1232: INFO server config env="map[HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:2048 OLLAMA_DEBUG:true OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://127.0.0.1:11434 OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/Users/sachinsuresh/.ollama/models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:0 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_SCHED_SPREAD:false http_proxy: https_proxy: no_proxy:]"
time=2025-04-25T09:31:34.842+05:30 level=INFO source=images.go:458 msg="total blobs: 29"
time=2025-04-25T09:31:34.843+05:30 level=INFO source=images.go:465 msg="total unused blobs removed: 0"
time=2025-04-25T09:31:34.843+05:30 level=INFO source=routes.go:1299 msg="Listening on 127.0.0.1:11434 (version 0.6.6)"
time=2025-04-25T09:31:34.843+05:30 level=DEBUG source=sched.go:107 msg="starting llm scheduler"
time=2025-04-25T09:31:34.901+05:30 level=INFO source=types.go:130 msg="inference compute" id=0 library=metal variant="" compute="" driver=0.0 name="" total="21.3 GiB" available="21.3 GiB"
time=2025-04-25T09:31:51.374+05:30 level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
time=2025-04-25T09:31:51.374+05:30 level=DEBUG source=sched.go:183 msg="updating default concurrency" OLLAMA_MAX_LOADED_MODELS=3 gpu_count=1
time=2025-04-25T09:31:51.379+05:30 level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
time=2025-04-25T09:31:51.383+05:30 level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
time=2025-04-25T09:31:51.384+05:30 level=DEBUG source=sched.go:226 msg="loading first model" model=/Users/sachinsuresh/.ollama/models/blobs/sha256-44d19d212d76a6f3fc442e8411fdb44ea6b67ceccfb00be4b4345c9a4cf813e8
time=2025-04-25T09:31:51.384+05:30 level=DEBUG source=memory.go:108 msg=evaluating library=metal gpu_count=1 available="[21.3 GiB]"
time=2025-04-25T09:31:51.384+05:30 level=WARN source=ggml.go:152 msg="key not found" key=granite.vision.block_count default=0
time=2025-04-25T09:31:51.384+05:30 level=WARN source=ggml.go:152 msg="key not found" key=granite.attention.key_length default=128
time=2025-04-25T09:31:51.384+05:30 level=WARN source=ggml.go:152 msg="key not found" key=granite.attention.value_length default=128
time=2025-04-25T09:31:51.384+05:30 level=INFO source=sched.go:722 msg="new model will fit in available VRAM in single GPU, loading" model=/Users/sachinsuresh/.ollama/models/blobs/sha256-44d19d212d76a6f3fc442e8411fdb44ea6b67ceccfb00be4b4345c9a4cf813e8 gpu=0 parallel=4 available=22906503168 required="13.7 GiB"
time=2025-04-25T09:31:51.385+05:30 level=INFO source=server.go:105 msg="system memory" total="32.0 GiB" free="11.8 GiB" free_swap="0 B"
time=2025-04-25T09:31:51.385+05:30 level=DEBUG source=memory.go:108 msg=evaluating library=metal gpu_count=1 available="[21.3 GiB]"
time=2025-04-25T09:31:51.385+05:30 level=WARN source=ggml.go:152 msg="key not found" key=granite.vision.block_count default=0
time=2025-04-25T09:31:51.385+05:30 level=WARN source=ggml.go:152 msg="key not found" key=granite.attention.key_length default=128
time=2025-04-25T09:31:51.385+05:30 level=WARN source=ggml.go:152 msg="key not found" key=granite.attention.value_length default=128
time=2025-04-25T09:31:51.385+05:30 level=INFO source=server.go:138 msg=offload library=metal layers.requested=-1 layers.model=41 layers.offload=41 layers.split="" memory.available="[21.3 GiB]" memory.gpu_overhead="0 B" memory.required.full="13.7 GiB" memory.required.partial="13.7 GiB" memory.required.kv="5.0 GiB" memory.required.allocations="[13.7 GiB]" memory.weights.total="4.6 GiB" memory.weights.repeating="4.4 GiB" memory.weights.nonrepeating="204.0 MiB" memory.graph.full="3.3 GiB" memory.graph.partial="3.3 GiB"
time=2025-04-25T09:31:51.386+05:30 level=DEBUG source=server.go:262 msg="compatible gpu libraries" compatible=[]
llama_model_load_from_file_impl: using device Metal (Apple M1 Pro) - 21845 MiB free
llama_model_loader: loaded meta data with 40 key-value pairs and 362 tensors from /Users/sachinsuresh/.ollama/models/blobs/sha256-44d19d212d76a6f3fc442e8411fdb44ea6b67ceccfb00be4b4345c9a4cf813e8 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = granite
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Granite 3.1 8b Instruct
llama_model_loader: - kv   3:                           general.finetune str              = instruct
llama_model_loader: - kv   4:                           general.basename str              = granite-3.1
llama_model_loader: - kv   5:                         general.size_label str              = 8B
llama_model_loader: - kv   6:                            general.license str              = apache-2.0
llama_model_loader: - kv   7:                   general.base_model.count u32              = 1
llama_model_loader: - kv   8:                  general.base_model.0.name str              = Granite 3.1 8b Base
llama_model_loader: - kv   9:          general.base_model.0.organization str              = Ibm Granite
llama_model_loader: - kv  10:              general.base_model.0.repo_url str              = https://huggingface.co/ibm-granite/gr...
llama_model_loader: - kv  11:                               general.tags arr[str,3]       = ["language", "granite-3.1", "text-gen...
llama_model_loader: - kv  12:                        granite.block_count u32              = 40
llama_model_loader: - kv  13:                     granite.context_length u32              = 131072
llama_model_loader: - kv  14:                   granite.embedding_length u32              = 4096
llama_model_loader: - kv  15:                granite.feed_forward_length u32              = 12800
llama_model_loader: - kv  16:               granite.attention.head_count u32              = 32
llama_model_loader: - kv  17:            granite.attention.head_count_kv u32              = 8
llama_model_loader: - kv  18:                     granite.rope.freq_base f32              = 10000000.000000
llama_model_loader: - kv  19:   granite.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  20:                          general.file_type u32              = 15
llama_model_loader: - kv  21:                         granite.vocab_size u32              = 49155
llama_model_loader: - kv  22:               granite.rope.dimension_count u32              = 128
llama_model_loader: - kv  23:                    granite.attention.scale f32              = 0.007812
llama_model_loader: - kv  24:                    granite.embedding_scale f32              = 12.000000
llama_model_loader: - kv  25:                     granite.residual_scale f32              = 0.220000
llama_model_loader: - kv  26:                        granite.logit_scale f32              = 16.000000
llama_model_loader: - kv  27:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  28:                         tokenizer.ggml.pre str              = refact
llama_model_loader: - kv  29:                      tokenizer.ggml.tokens arr[str,49155]   = ["<|end_of_text|>", "<fim_prefix>", "...
llama_model_loader: - kv  30:                  tokenizer.ggml.token_type arr[i32,49155]   = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...
llama_model_loader: - kv  31:                      tokenizer.ggml.merges arr[str,48891]   = ["Ġ Ġ", "ĠĠ ĠĠ", "ĠĠĠĠ ĠĠ...
llama_model_loader: - kv  32:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  33:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  34:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  35:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  36:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  37:                    tokenizer.chat_template str              = {%- if messages[0]['role'] == 'system...
llama_model_loader: - kv  38:            tokenizer.ggml.add_space_prefix bool             = false
llama_model_loader: - kv  39:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   81 tensors
llama_model_loader: - type q8_0:    1 tensors
llama_model_loader: - type q4_K:  240 tensors
llama_model_loader: - type q6_K:   40 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.65 GiB (4.89 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:  49152 '<|start_of_role|>' is not marked as EOG
load: control token:      6 '<gh_stars>' is not marked as EOG
load: control token:      0 '<|end_of_text|>' is not marked as EOG
load: control token:  49153 '<|end_of_role|>' is not marked as EOG
load: control token:      3 '<fim_suffix>' is not marked as EOG
load: control token:      1 '<fim_prefix>' is not marked as EOG
load: control token:      9 '<issue_closed>' is not marked as EOG
load: control token:     13 '<jupyter_output>' is not marked as EOG
load: control token:      7 '<issue_start>' is not marked as EOG
load: control token:      5 '<filename>' is not marked as EOG
load: control token:     18 '<reponame>' is not marked as EOG
load: control token:      2 '<fim_middle>' is not marked as EOG
load: control token:     15 '<commit_before>' is not marked as EOG
load: control token:     17 '<commit_after>' is not marked as EOG
load: control token:     14 '<empty_output>' is not marked as EOG
load: control token:     11 '<jupyter_text>' is not marked as EOG
load: control token:      4 '<fim_pad>' is not marked as EOG
load: control token:  49154 '<|tool_call|>' is not marked as EOG
load: control token:      8 '<issue_comment>' is not marked as EOG
load: control token:     12 '<jupyter_code>' is not marked as EOG
load: control token:     10 '<jupyter_start>' is not marked as EOG
load: control token:     16 '<commit_msg>' is not marked as EOG
load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
load: special tokens cache size = 22
load: token to piece cache size = 0.2826 MB
print_info: arch             = granite
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 8.17 B
print_info: general.name     = Granite 3.1 8b Instruct
print_info: f_embedding_scale = 0.000000
print_info: f_residual_scale  = 0.000000
print_info: f_attention_scale = 0.000000
print_info: vocab type       = BPE
print_info: n_vocab          = 49155
print_info: n_merges         = 48891
print_info: BOS token        = 0 '<|end_of_text|>'
print_info: EOS token        = 0 '<|end_of_text|>'
print_info: UNK token        = 0 '<|end_of_text|>'
print_info: PAD token        = 0 '<|end_of_text|>'
print_info: LF token         = 203 'Ċ'
print_info: EOG token        = 0 '<|end_of_text|>'
print_info: max token length = 512
llama_model_load: vocab only - skipping tensors
time=2025-04-25T09:31:51.446+05:30 level=INFO source=server.go:405 msg="starting llama server" cmd="/Applications/Ollama.app/Contents/Resources/ollama runner --model /Users/sachinsuresh/.ollama/models/blobs/sha256-44d19d212d76a6f3fc442e8411fdb44ea6b67ceccfb00be4b4345c9a4cf813e8 --ctx-size 32768 --batch-size 512 --n-gpu-layers 41 --verbose --threads 8 --parallel 4 --port 55290"
time=2025-04-25T09:31:51.447+05:30 level=DEBUG source=server.go:423 msg=subprocess environment="[PATH=/Users/sachinsuresh/Documents/IBM-Code-Assist-Web-UI/path/to/venv/bin:/Users/sachinsuresh/.nvm/versions/node/v22.13.0/bin:/Library/Java/JavaVirtualMachines/jdk1.8.0_311.jdk/Contents/Home/bin:/opt/homebrew/bin:/opt/homebrew/sbin:/usr/local/bin:/System/Cryptexes/App/usr/bin:/usr/bin:/bin:/usr/sbin:/sbin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/local/bin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/bin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/appleinternal/bin:/opt/podman/bin:/Users/sachinsuresh/.nvm/versions/node/v22.13.0/bin:/Library/Java/JavaVirtualMachines/jdk1.8.0_311.jdk/Contents/Home/bin:/Users/sachinsuresh/.local/bin:/Users/sachinsuresh/.vscode/extensions/ms-python.debugpy-2025.6.0-darwin-arm64/bundled/scripts/noConfigScripts:/Users/sachinsuresh/Library/Application Support/Code/User/globalStorage/github.copilot-chat/debugCommand:/Users/sachinsuresh/.local/bin:/Users/sachinsuresh/.local/bin DYLD_LIBRARY_PATH=/Applications/Ollama.app/Contents/Resources]"
time=2025-04-25T09:31:51.449+05:30 level=INFO source=sched.go:451 msg="loaded runners" count=1
time=2025-04-25T09:31:51.449+05:30 level=INFO source=server.go:580 msg="waiting for llama runner to start responding"
time=2025-04-25T09:31:51.450+05:30 level=INFO source=server.go:614 msg="waiting for server to become available" status="llm server error"
time=2025-04-25T09:31:51.467+05:30 level=INFO source=runner.go:853 msg="starting go runner"
time=2025-04-25T09:31:51.467+05:30 level=DEBUG source=ggml.go:99 msg="ggml backend load all from path" path=/Applications/Ollama.app/Contents/Resources
time=2025-04-25T09:31:51.471+05:30 level=INFO source=ggml.go:109 msg=system Metal.0.EMBED_LIBRARY=1 CPU.0.NEON=1 CPU.0.ARM_FMA=1 CPU.0.FP16_VA=1 CPU.0.DOTPROD=1 CPU.0.LLAMAFILE=1 CPU.0.ACCELERATE=1 compiler=cgo(clang)
time=2025-04-25T09:31:51.472+05:30 level=INFO source=runner.go:913 msg="Server listening on 127.0.0.1:55290"
llama_model_load_from_file_impl: using device Metal (Apple M1 Pro) - 21845 MiB free
llama_model_loader: loaded meta data with 40 key-value pairs and 362 tensors from /Users/sachinsuresh/.ollama/models/blobs/sha256-44d19d212d76a6f3fc442e8411fdb44ea6b67ceccfb00be4b4345c9a4cf813e8 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = granite
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Granite 3.1 8b Instruct
llama_model_loader: - kv   3:                           general.finetune str              = instruct
llama_model_loader: - kv   4:                           general.basename str              = granite-3.1
llama_model_loader: - kv   5:                         general.size_label str              = 8B
llama_model_loader: - kv   6:                            general.license str              = apache-2.0
llama_model_loader: - kv   7:                   general.base_model.count u32              = 1
llama_model_loader: - kv   8:                  general.base_model.0.name str              = Granite 3.1 8b Base
llama_model_loader: - kv   9:          general.base_model.0.organization str              = Ibm Granite
llama_model_loader: - kv  10:              general.base_model.0.repo_url str              = https://huggingface.co/ibm-granite/gr...
llama_model_loader: - kv  11:                               general.tags arr[str,3]       = ["language", "granite-3.1", "text-gen...
llama_model_loader: - kv  12:                        granite.block_count u32              = 40
llama_model_loader: - kv  13:                     granite.context_length u32              = 131072
llama_model_loader: - kv  14:                   granite.embedding_length u32              = 4096
llama_model_loader: - kv  15:                granite.feed_forward_length u32              = 12800
llama_model_loader: - kv  16:               granite.attention.head_count u32              = 32
llama_model_loader: - kv  17:            granite.attention.head_count_kv u32              = 8
llama_model_loader: - kv  18:                     granite.rope.freq_base f32              = 10000000.000000
llama_model_loader: - kv  19:   granite.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  20:                          general.file_type u32              = 15
llama_model_loader: - kv  21:                         granite.vocab_size u32              = 49155
llama_model_loader: - kv  22:               granite.rope.dimension_count u32              = 128
llama_model_loader: - kv  23:                    granite.attention.scale f32              = 0.007812
llama_model_loader: - kv  24:                    granite.embedding_scale f32              = 12.000000
llama_model_loader: - kv  25:                     granite.residual_scale f32              = 0.220000
llama_model_loader: - kv  26:                        granite.logit_scale f32              = 16.000000
llama_model_loader: - kv  27:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  28:                         tokenizer.ggml.pre str              = refact
llama_model_loader: - kv  29:                      tokenizer.ggml.tokens arr[str,49155]   = ["<|end_of_text|>", "<fim_prefix>", "...
llama_model_loader: - kv  30:                  tokenizer.ggml.token_type arr[i32,49155]   = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...
llama_model_loader: - kv  31:                      tokenizer.ggml.merges arr[str,48891]   = ["Ġ Ġ", "ĠĠ ĠĠ", "ĠĠĠĠ ĠĠ...
llama_model_loader: - kv  32:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  33:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  34:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  35:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  36:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  37:                    tokenizer.chat_template str              = {%- if messages[0]['role'] == 'system...
llama_model_loader: - kv  38:            tokenizer.ggml.add_space_prefix bool             = false
llama_model_loader: - kv  39:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   81 tensors
llama_model_loader: - type q8_0:    1 tensors
llama_model_loader: - type q4_K:  240 tensors
llama_model_loader: - type q6_K:   40 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.65 GiB (4.89 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:  49152 '<|start_of_role|>' is not marked as EOG
load: control token:      6 '<gh_stars>' is not marked as EOG
load: control token:      0 '<|end_of_text|>' is not marked as EOG
load: control token:  49153 '<|end_of_role|>' is not marked as EOG
load: control token:      3 '<fim_suffix>' is not marked as EOG
load: control token:      1 '<fim_prefix>' is not marked as EOG
load: control token:      9 '<issue_closed>' is not marked as EOG
load: control token:     13 '<jupyter_output>' is not marked as EOG
load: control token:      7 '<issue_start>' is not marked as EOG
load: control token:      5 '<filename>' is not marked as EOG
load: control token:     18 '<reponame>' is not marked as EOG
load: control token:      2 '<fim_middle>' is not marked as EOG
load: control token:     15 '<commit_before>' is not marked as EOG
load: control token:     17 '<commit_after>' is not marked as EOG
load: control token:     14 '<empty_output>' is not marked as EOG
load: control token:     11 '<jupyter_text>' is not marked as EOG
load: control token:      4 '<fim_pad>' is not marked as EOG
load: control token:  49154 '<|tool_call|>' is not marked as EOG
load: control token:      8 '<issue_comment>' is not marked as EOG
load: control token:     12 '<jupyter_code>' is not marked as EOG
load: control token:     10 '<jupyter_start>' is not marked as EOG
load: control token:     16 '<commit_msg>' is not marked as EOG
load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
load: special tokens cache size = 22
load: token to piece cache size = 0.2826 MB
print_info: arch             = granite
print_info: vocab_only       = 0
print_info: n_ctx_train      = 131072
print_info: n_embd           = 4096
print_info: n_layer          = 40
print_info: n_head           = 32
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 4
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 1.6e+01
print_info: f_attn_scale     = 7.8e-03
print_info: n_ff             = 12800
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 0
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 131072
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 3B
print_info: model params     = 8.17 B
print_info: general.name     = Granite 3.1 8b Instruct
print_info: f_embedding_scale = 12.000000
print_info: f_residual_scale  = 0.220000
print_info: f_attention_scale = 0.007812
print_info: vocab type       = BPE
print_info: n_vocab          = 49155
print_info: n_merges         = 48891
print_info: BOS token        = 0 '<|end_of_text|>'
print_info: EOS token        = 0 '<|end_of_text|>'
print_info: UNK token        = 0 '<|end_of_text|>'
print_info: PAD token        = 0 '<|end_of_text|>'
print_info: LF token         = 203 'Ċ'
print_info: EOG token        = 0 '<|end_of_text|>'
print_info: max token length = 512
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal, is_swa = 0
load_tensors: layer   1 assigned to device Metal, is_swa = 0
load_tensors: layer   2 assigned to device Metal, is_swa = 0
load_tensors: layer   3 assigned to device Metal, is_swa = 0
load_tensors: layer   4 assigned to device Metal, is_swa = 0
load_tensors: layer   5 assigned to device Metal, is_swa = 0
load_tensors: layer   6 assigned to device Metal, is_swa = 0
load_tensors: layer   7 assigned to device Metal, is_swa = 0
load_tensors: layer   8 assigned to device Metal, is_swa = 0
load_tensors: layer   9 assigned to device Metal, is_swa = 0
load_tensors: layer  10 assigned to device Metal, is_swa = 0
load_tensors: layer  11 assigned to device Metal, is_swa = 0
load_tensors: layer  12 assigned to device Metal, is_swa = 0
load_tensors: layer  13 assigned to device Metal, is_swa = 0
load_tensors: layer  14 assigned to device Metal, is_swa = 0
load_tensors: layer  15 assigned to device Metal, is_swa = 0
load_tensors: layer  16 assigned to device Metal, is_swa = 0
load_tensors: layer  17 assigned to device Metal, is_swa = 0
load_tensors: layer  18 assigned to device Metal, is_swa = 0
load_tensors: layer  19 assigned to device Metal, is_swa = 0
load_tensors: layer  20 assigned to device Metal, is_swa = 0
load_tensors: layer  21 assigned to device Metal, is_swa = 0
load_tensors: layer  22 assigned to device Metal, is_swa = 0
load_tensors: layer  23 assigned to device Metal, is_swa = 0
load_tensors: layer  24 assigned to device Metal, is_swa = 0
load_tensors: layer  25 assigned to device Metal, is_swa = 0
load_tensors: layer  26 assigned to device Metal, is_swa = 0
load_tensors: layer  27 assigned to device Metal, is_swa = 0
load_tensors: layer  28 assigned to device Metal, is_swa = 0
load_tensors: layer  29 assigned to device Metal, is_swa = 0
load_tensors: layer  30 assigned to device Metal, is_swa = 0
load_tensors: layer  31 assigned to device Metal, is_swa = 0
load_tensors: layer  32 assigned to device Metal, is_swa = 0
load_tensors: layer  33 assigned to device Metal, is_swa = 0
load_tensors: layer  34 assigned to device Metal, is_swa = 0
load_tensors: layer  35 assigned to device Metal, is_swa = 0
load_tensors: layer  36 assigned to device Metal, is_swa = 0
load_tensors: layer  37 assigned to device Metal, is_swa = 0
load_tensors: layer  38 assigned to device Metal, is_swa = 0
load_tensors: layer  39 assigned to device Metal, is_swa = 0
load_tensors: layer  40 assigned to device Metal, is_swa = 0
ggml_backend_metal_log_allocated_size: allocated buffer, size =  4758.73 MiB, ( 4758.80 / 21845.34)
load_tensors: offloading 40 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 41/41 layers to GPU
load_tensors:   CPU_Mapped model buffer size =   204.01 MiB
load_tensors: Metal_Mapped model buffer size =  4758.72 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 4
llama_context: n_ctx         = 32768
llama_context: n_ctx_per_seq = 8192
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 10000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M1 Pro
ggml_metal_init: picking default device: Apple M1 Pro
ggml_metal_load_library: using embedded metal library
ggml_metal_init: GPU name:   Apple M1 Pro
ggml_metal_init: GPU family: MTLGPUFamilyApple7  (1007)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = false
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = false
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 22906.50 MB
ggml_metal_init: loaded kernel_add                                    0x15b447270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x15b446220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x15a715dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x15a716780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x15a716e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x15a715ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x15a717e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x15a7185a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x15a718db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x15a717750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x15a7196e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x15a719ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x15b449090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x15b4492c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x15b449950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x15b44aed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x15b44b900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x15b44c380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x15b44cdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x15b44dab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x15b44e500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x15b44ef20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x15b44f980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x15b450620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x15b450f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x15b451780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x15b452520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x15b452e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x15b453670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x15b4519e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x15b454210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x15b454950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x15b454ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x15b4556f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x15a71a690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x15a71ae70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x15a71baf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x15a71c2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x15a71ca50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x15a71b430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x15a71d680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x15a71e480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x15b507df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x15b508240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x15b508a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x15a71ec60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x15a71ee90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x15a71f260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x15a71fb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x15a720610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x15a720df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x15a7215d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x15a721db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x15a722b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_l2_norm                                0x15a723340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x15a723c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x15a724ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x15a725270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x15a725a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rwkv_wkv6_f32                          0x15a726310 | th_max =  448 | th_width =   32
ggml_metal_init: loaded kernel_rwkv_wkv7_f32                          0x15a726b40 | th_max =  448 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x15a727280 | th_max = 1024 | th_width =   32
ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x15a727ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x15a7282f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x15a728b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x15a729310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x15a729af0 | th_max =  640 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x15a72a2d0 | th_max =  832 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x15a72aab0 | th_max =  640 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x15a72b290 | th_max =  576 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x15a72ba70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x15a72c280 | th_max =  896 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x15b455ae0 | th_max =  640 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x15b456350 | th_max =  512 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x15a72c670 | th_max =  576 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x15a72ca60 | th_max =  832 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x15b456580 | th_max =  704 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x15b456970 | th_max =  640 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x15b456d60 | th_max =  640 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x15b4575f0 | th_max =  768 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x15a72ce50 | th_max =  704 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x15a72d7e0 | th_max =  640 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x15a72dc10 | th_max =  640 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x15a72e4c0 | th_max =  640 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x15a72ed00 | th_max =  704 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x15b457e60 | th_max =  640 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x15b458680 | th_max =  640 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x15b459550 | th_max =  640 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x15a72f540 | th_max =  704 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x15a730070 | th_max =  640 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x15a730f00 | th_max =  640 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x15a731750 | th_max =  832 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x15a731f90 | th_max =  704 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x15a7327d0 | th_max =  640 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x15a733010 | th_max =  640 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x15a733850 | th_max =  832 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x15b458df0 | th_max =  704 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x15b459f50 | th_max =  640 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x15b45ae00 | th_max =  640 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x15b45b650 | th_max =  704 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x15b45be90 | th_max =  704 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x15a730580 | th_max =  640 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x15a734090 | th_max =  640 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x15b45a660 | th_max =  768 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x15b45c890 | th_max =  704 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x15b45d6e0 | th_max =  640 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x15b45df20 | th_max =  640 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x15b45cc80 | th_max =  832 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x15b45e900 | th_max =  704 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x15b45f140 | th_max =  640 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x15b45f980 | th_max =  640 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x15b4601d0 | th_max =  640 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x15b460a10 | th_max =  576 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x15b4611f0 | th_max =  576 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x15b4619e0 | th_max =  576 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x15b462190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x15b462980 | th_max =  832 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x15b4631a0 | th_max =  704 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x15b463990 | th_max =  768 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x15b4641d0 | th_max =  640 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x15b464a20 | th_max =  704 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x15b465260 | th_max =  448 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x15b465aa0 | th_max =  576 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x15b4665a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x15b466de0 | th_max =  896 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x15b467610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x15b467e50 | th_max = 1024 | th_width =   32
ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x15b4686b0 | th_max =  832 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x15b468ee0 | th_max =  832 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x15b469d90 | th_max =  576 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x15b46a5d0 | th_max =  576 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x15b46ae30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x15b46b690 | th_max =  576 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x15b46bed0 | th_max =  576 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x15a734700 | th_max =  576 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x15a734fd0 | th_max =  576 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x15a735ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x15a736720 | th_max =  832 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x15a736f80 | th_max =  640 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x15a7377e0 | th_max =  704 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x15a738040 | th_max =  640 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x15a7388a0 | th_max =  640 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x15a739100 | th_max =  448 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x15a739940 | th_max =  576 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x15a73a150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x15a73a9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x15b5092e0 | th_max =  832 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x15b50a9c0 | th_max =  832 | th_width =   32
ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x15a7353c0 | th_max =  768 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x15a7357b0 | th_max =  768 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x15b5085e0 | th_max =  768 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x15a73baa0 | th_max =  768 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x15a73c3d0 | th_max =  832 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x15a73cee0 | th_max =  832 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x15a73c7c0 | th_max =  832 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x15a73d850 | th_max =  832 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x15a73e030 | th_max =  768 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x15a73e7f0 | th_max =  832 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x15a73efb0 | th_max =  768 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x15a73f760 | th_max =  832 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x15a73ff40 | th_max =  832 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x15a740760 | th_max =  832 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x15a740f80 | th_max =  832 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x15a7417c0 | th_max =  832 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x15a742030 | th_max =  832 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x15a742840 | th_max =  832 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x15a743080 | th_max =  832 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x15a7438f0 | th_max =  832 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x15a744110 | th_max =  832 | th_width =   32
ggml_metal_init: skipping kernel_mul_mm_id_bf16_f32                (not supported)
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x15a744960 | th_max =  832 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x15a7451d0 | th_max =  832 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x15a745a10 | th_max =  768 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x15a746250 | th_max =  768 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x15a746ab0 | th_max =  832 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x15a747310 | th_max =  832 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x15a747b40 | th_max =  832 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x15a748390 | th_max =  832 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x15a748bf0 | th_max =  832 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x15a749430 | th_max =  832 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x15a749c90 | th_max =  832 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x15a74a4d0 | th_max =  832 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x15a74ad30 | th_max =  832 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x15a74b570 | th_max =  832 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x15a74bdd0 | th_max =  832 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x15a74c610 | th_max =  832 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x15a74ce70 | th_max =  832 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x15a74d6b0 | th_max =  832 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x15a74df10 | th_max =  832 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x15a74e750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x15a74efb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x15a74f790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x15a74ff70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x15a750750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x15a750f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x15a751710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x15a751ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x15a7526d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x15a752eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x15a753710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x15a753f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x15a754cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_unpad_f32                              0x15a754f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x15a755cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x15a755ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x15a756cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x15a757470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x15a758090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x15a758890 | th_max =  640 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x15a759130 | th_max =  640 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x15a759990 | th_max =  640 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x15a75a1f0 | th_max =  576 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x15a75aa50 | th_max =  576 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h192                0x15a75b270 | th_max =  576 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_hk192_hv128         0x15a75bad0 | th_max =  576 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x15a75c330 | th_max =  832 | th_width =   32
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x15a75cb90 | th_max =  640 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x15a75d3d0 | th_max =  768 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x15b46c710 | th_max =  768 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x15b46cb00 | th_max =  768 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x15a75dc30 | th_max =  704 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h192               0x15a75e020 | th_max =  704 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_hk192_hv128        0x15a75ecc0 | th_max =  704 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x15a75f550 | th_max =  704 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x15a760070 | th_max =  640 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x15a7608d0 | th_max =  768 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x15a75e410 | th_max =  768 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x15a761110 | th_max =  768 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x15a761ef0 | th_max =  704 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h192               0x1509933f0 | th_max =  704 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_hk192_hv128        0x15a762990 | th_max =  704 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x15a762e80 | th_max =  704 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x15b46d7f0 | th_max =  576 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x15b46dbe0 | th_max =  640 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x15b46eb90 | th_max =  640 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x15b46f3f0 | th_max =  640 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x15a761760 | th_max =  640 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h192               0x15a763fe0 | th_max =  640 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_hk192_hv128        0x15b46fee0 | th_max =  640 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x15b4702d0 | th_max =  640 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x15a763600 | th_max =  576 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x15a764fd0 | th_max =  640 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x15a765980 | th_max =  640 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x15a766240 | th_max =  640 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x15a764940 | th_max =  640 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h192               0x15a766c20 | th_max =  640 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_hk192_hv128        0x15a767480 | th_max =  640 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x15a767ce0 | th_max =  640 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x15a768540 | th_max =  640 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x15a768da0 | th_max =  768 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x15a7695e0 | th_max =  768 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x15a769e40 | th_max =  768 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x15a76a680 | th_max =  768 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h192               0x15a76aef0 | th_max =  768 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_hk192_hv128        0x15b46e320 | th_max =  768 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x15b4713c0 | th_max =  768 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x15b471d50 | th_max = 1024 | th_width =   32
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x15b471ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x15b472610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x15b473000 | th_max =  832 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x15b473860 | th_max =  832 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x15b4740c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h192            0x15b474920 | th_max =  832 | th_width =   32
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h192           0x15b475180 | th_max =  896 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h192           0x15b4759e0 | th_max =  896 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h192           0x15b476240 | th_max =  768 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h192           0x15b476aa0 | th_max =  704 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h192           0x15b477300 | th_max =  896 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_hk192_hv128      0x15b477b60 | th_max =  896 | th_width =   32
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_hk192_hv128      0x15b4783c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_hk192_hv128      0x15b478c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_hk192_hv128      0x15b479490 | th_max =  832 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_hk192_hv128      0x15b479d00 | th_max =  768 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_hk192_hv128      0x15b47a560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x15b47adc0 | th_max =  768 | th_width =   32
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x15b50b520 | th_max =  832 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x15b50c8a0 | th_max =  768 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x15b50cad0 | th_max =  640 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x15b50d230 | th_max =  576 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x15a76bf00 | th_max =  768 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x15a6c9ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x15b470c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x15b47be80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x15b47c270 | th_max = 1024 | th_width =   32
ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x15b47d450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x15b47dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x15b47e3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x15b47eb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x15b47f380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x15b47fb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x15a76c6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x15a765500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x15a76d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x15a76d7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x15b50d9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x15b50e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x15b50f0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x15b50f8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x15b510090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x15b510870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x15b47cca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x15b4809a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x15b4812e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x15b481d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x15b482880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x15b4838b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x15b484300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_neg                                    0x15b484d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x15b10fd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x15b110450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x15b110680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x15b110f00 | th_max = 1024 | th_width =   32
set_abort_callback: call
llama_context:        CPU  output buffer size =     0.81 MiB
llama_context: n_ctx = 32768
llama_context: n_ctx = 32768 (padded)
init: kv_size = 32768, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 40, can_shift = 1
init: layer   0: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024, dev = Metal
init: layer   1: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024, dev = Metal
init: layer   2: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024, dev = Metal
init: layer   3: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024, dev = Metal
init: layer   4: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024, dev = Metal
init: layer   5: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024, dev = Metal
init: layer   6: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024, dev = Metal
init: layer   7: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024, dev = Metal
init: layer   8: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024, dev = Metal
init: layer   9: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024, dev = Metal
init: layer  10: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024, dev = Metal
init: layer  11: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024, dev = Metal
init: layer  12: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024, dev = Metal
init: layer  13: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024, dev = Metal
init: layer  14: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024, dev = Metal
init: layer  15: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024, dev = Metal
init: layer  16: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024, dev = Metal
init: layer  17: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024, dev = Metal
init: layer  18: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024, dev = Metal
init: layer  19: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024, dev = Metal
init: layer  20: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024, dev = Metal
init: layer  21: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024, dev = Metal
init: layer  22: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024, dev = Metal
init: layer  23: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024, dev = Metal
init: layer  24: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024, dev = Metal
init: layer  25: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024, dev = Metal
init: layer  26: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024, dev = Metal
init: layer  27: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024, dev = Metal
init: layer  28: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024, dev = Metal
init: layer  29: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024, dev = Metal
init: layer  30: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024, dev = Metal
init: layer  31: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024, dev = Metal
init: layer  32: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024, dev = Metal
init: layer  33: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024, dev = Metal
init: layer  34: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024, dev = Metal
init: layer  35: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024, dev = Metal
init: layer  36: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024, dev = Metal
init: layer  37: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024, dev = Metal
init: layer  38: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024, dev = Metal
init: layer  39: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024, dev = Metal
time=2025-04-25T09:31:51.702+05:30 level=INFO source=server.go:614 msg="waiting for server to become available" status="llm server loading model"
time=2025-04-25T09:31:51.702+05:30 level=DEBUG source=server.go:625 msg="model load progress 1.00"
time=2025-04-25T09:31:51.953+05:30 level=DEBUG source=server.go:628 msg="model load completed, waiting for server to become available" status="llm server loading model"
init:      Metal KV buffer size =  5120.00 MiB
llama_context: KV self size  = 5120.00 MiB, K (f16): 2560.00 MiB, V (f16): 2560.00 MiB
llama_context: enumerating backends
llama_context: backend_ptrs.size() = 2
llama_context: max_nodes = 65536
llama_context: worst-case: n_tokens = 512, n_seqs = 1, n_outputs = 0
llama_context: reserving graph for n_tokens = 512, n_seqs = 1
llama_context: reserving graph for n_tokens = 1, n_seqs = 1
llama_context: reserving graph for n_tokens = 512, n_seqs = 1
llama_context:      Metal compute buffer size =  2144.00 MiB
llama_context:        CPU compute buffer size =    72.01 MiB
llama_context: graph nodes  = 1448
llama_context: graph splits = 2
time=2025-04-25T09:31:52.708+05:30 level=INFO source=server.go:619 msg="llama runner started in 1.26 seconds"
time=2025-04-25T09:31:52.708+05:30 level=DEBUG source=sched.go:464 msg="finished setting up runner" model=/Users/sachinsuresh/.ollama/models/blobs/sha256-44d19d212d76a6f3fc442e8411fdb44ea6b67ceccfb00be4b4345c9a4cf813e8
time=2025-04-25T09:31:52.710+05:30 level=DEBUG source=routes.go:1523 msg="chat request" images=0 prompt="<|start_of_role|>system<|end_of_role|><important_rules>\n  Always include the language and file name in the info string when you write code blocks. If you are editing \"src/main.py\" for example, your code block should start with '```python src/main.py'.\n</important_rules><|end_of_text|> You are a helpful AI assistant.<|end_of_text|>\n<|start_of_role|>user<|end_of_role|>answer brief: What is AWS?<|end_of_text|>\n<|start_of_role|>assistant<|end_of_role|>"
time=2025-04-25T09:31:52.712+05:30 level=DEBUG source=cache.go:104 msg="loading cache slot" id=0 cache=0 prompt=86 used=0 remaining=86
[GIN] 2025/04/25 - 09:32:13 | 200 | 22.380772875s |       127.0.0.1 | POST     "/api/chat"
time=2025-04-25T09:32:13.745+05:30 level=DEBUG source=sched.go:468 msg="context for request finished"
time=2025-04-25T09:32:13.745+05:30 level=DEBUG source=sched.go:341 msg="runner with non-zero duration has gone idle, adding timer" modelPath=/Users/sachinsuresh/.ollama/models/blobs/sha256-44d19d212d76a6f3fc442e8411fdb44ea6b67ceccfb00be4b4345c9a4cf813e8 duration=30m0s
time=2025-04-25T09:32:13.745+05:30 level=DEBUG source=sched.go:359 msg="after processing request finished event" modelPath=/Users/sachinsuresh/.ollama/models/blobs/sha256-44d19d212d76a6f3fc442e8411fdb44ea6b67ceccfb00be4b4345c9a4cf813e8 refCount=0
time=2025-04-25T09:32:13.793+05:30 level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
time=2025-04-25T09:32:13.795+05:30 level=DEBUG source=sched.go:577 msg="evaluating already loaded" model=/Users/sachinsuresh/.ollama/models/blobs/sha256-44d19d212d76a6f3fc442e8411fdb44ea6b67ceccfb00be4b4345c9a4cf813e8
time=2025-04-25T09:32:13.797+05:30 level=DEBUG source=routes.go:1523 msg="chat request" images=0 prompt="<|start_of_role|>system<|end_of_role|>Knowledge Cutoff Date: April 2024.\nYou are Granite, developed by IBM.<|end_of_text|> You are a helpful AI assistant.<|end_of_text|>\n<|start_of_role|>user<|end_of_role|>Given the following... please reply with a title for the chat that is 3-4 words in length, all words used should be directly related to the content of the chat, avoid using verbs unless they are directly related to the content of the chat, no additional text or explanation, you don't need ending punctuation.\n\nAWS, or Amazon Web Services, is a comprehensive, evolving cloud computing platform provided by tech giant Amazon. It offers various services including computing power, storage options, content delivery and other functionality to help businesses scale and grow. These services are delivered from data centers around the world, providing a reliable, scalable, and secure environment for applications and services.\n\nHere's an example of how you might define AWS in Python:\n\n\n\nThis code creates a simple Python class `Aws` which has a list of AWS services. The method `describe` returns a string describing what AWS is.<|end_of_text|>\n<|start_of_role|>assistant<|end_of_role|>"
time=2025-04-25T09:32:13.800+05:30 level=DEBUG source=cache.go:104 msg="loading cache slot" id=0 cache=301 prompt=233 used=3 remaining=230
[GIN] 2025/04/25 - 09:32:15 | 200 |     1.727452s |       127.0.0.1 | POST     "/api/chat"
time=2025-04-25T09:32:15.510+05:30 level=DEBUG source=sched.go:409 msg="context for request finished"
time=2025-04-25T09:32:15.510+05:30 level=DEBUG source=sched.go:341 msg="runner with non-zero duration has gone idle, adding timer" modelPath=/Users/sachinsuresh/.ollama/models/blobs/sha256-44d19d212d76a6f3fc442e8411fdb44ea6b67ceccfb00be4b4345c9a4cf813e8 duration=30m0s
time=2025-04-25T09:32:15.510+05:30 level=DEBUG source=sched.go:359 msg="after processing request finished event" modelPath=/Users/sachinsuresh/.ollama/models/blobs/sha256-44d19d212d76a6f3fc442e8411fdb44ea6b67ceccfb00be4b4345c9a4cf813e8 refCount=0
[GIN] 2025/04/25 - 09:32:51 | 200 |      31.542µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/04/25 - 09:32:51 | 200 |     110.625µs |       127.0.0.1 | GET      "/api/ps"
time=2025-04-25T09:32:56.543+05:30 level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
time=2025-04-25T09:32:56.544+05:30 level=DEBUG source=sched.go:577 msg="evaluating already loaded" model=/Users/sachinsuresh/.ollama/models/blobs/sha256-44d19d212d76a6f3fc442e8411fdb44ea6b67ceccfb00be4b4345c9a4cf813e8
time=2025-04-25T09:32:56.547+05:30 level=DEBUG source=routes.go:1523 msg="chat request" images=0 prompt="<|start_of_role|>system<|end_of_role|><important_rules>\n  Always include the language and file name in the info string when you write code blocks. If you are editing \"src/main.py\" for example, your code block should start with '```python src/main.py'.\n</important_rules><|end_of_text|> You are a helpful AI assistant.<|end_of_text|>\n<|start_of_role|>user<|end_of_role|>answer brief: What is AWS?<|end_of_text|>\n<|start_of_role|>assistant<|end_of_role|>AWS, or Amazon Web Services, is a comprehensive, evolving cloud computing platform provided by tech giant Amazon. It offers various services including computing power, storage options, content delivery and other functionality to help businesses scale and grow. These services are delivered from data centers around the world, providing a reliable, scalable, and secure environment for applications and services.\n\nHere's an example of how you might define AWS in Python:\n\n```python src/aws_definition.py\nclass Aws:\n    def __init__(self):\n        self.services = ['EC2', 'S3', 'RDS', 'Lambda']\n\n    def describe(self):\n        description = \"AWS, or Amazon Web Services, is a cloud computing platform that offers a variety of services including computing power, storage options, and more to assist businesses in scaling and growing.\"\n        return description\n```\n\nThis code creates a simple Python class `Aws` which has a list of AWS services. The method `describe` returns a string describing what AWS is.<|end_of_text|>\n<|start_of_role|>user<|end_of_role|>What is java?<|end_of_text|>\n<|start_of_role|>assistant<|end_of_role|>"
time=2025-04-25T09:32:56.549+05:30 level=DEBUG source=cache.go:104 msg="loading cache slot" id=0 cache=241 prompt=315 used=3 remaining=312
[GIN] 2025/04/25 - 09:33:12 | 200 |  15.92967025s |       127.0.0.1 | POST     "/api/chat"
time=2025-04-25T09:33:12.464+05:30 level=DEBUG source=sched.go:409 msg="context for request finished"
time=2025-04-25T09:33:12.464+05:30 level=DEBUG source=sched.go:341 msg="runner with non-zero duration has gone idle, adding timer" modelPath=/Users/sachinsuresh/.ollama/models/blobs/sha256-44d19d212d76a6f3fc442e8411fdb44ea6b67ceccfb00be4b4345c9a4cf813e8 duration=30m0s
time=2025-04-25T09:33:12.464+05:30 level=DEBUG source=sched.go:359 msg="after processing request finished event" modelPath=/Users/sachinsuresh/.ollama/models/blobs/sha256-44d19d212d76a6f3fc442e8411fdb44ea6b67ceccfb00be4b4345c9a4cf813e8 refCount=0
